{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b6c510-e17a-4228-b5a5-b18a664a2fa2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# *Siglo XXI*\n",
    "##Challenge Data engineer \n",
    "###Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddaa56ec-7a1a-43f1-9033-f407458afefd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Contexto\n",
    "\n",
    "El siguiente challenge está pensado para que el desarrollador utilice las herramientas que crea necesarias para llevarlo a cabo. Para el desarrollo sobre las herramientas de azure, se debe utilizar la evaluación gratuita de 30 días que micrsoft ofrece con cualquier cuenta de email.\n",
    "\n",
    "Parte 1: Extracción y Transformación de Datos con PySpark\n",
    "\n",
    "Extracción de Datos desde una API:\n",
    "\n",
    "- Utilizar Spark para extraer datos de una API pública de tu elección.\n",
    "- Aplica transformaciones necesarias para preparar los datos para su posterior análisis (ej: procesar archivos json a tablas).\n",
    "- Consolidar un archivo único con el resultado de la extracción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82bdf314-7a0e-4e7a-abda-35817ef51512",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para realizar este examen se decidió utilizar la API de CoinGecko que brinda información sobre criptomonedas.\n",
    "\n",
    "Fuente: https://www.coingecko.com/api/documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "914fc9e7-4c7a-484b-b3ca-53cc6ea2fcaa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###**ETL realizado con pySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c9af83-bf19-415d-960c-b6710aba4086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/sigloXXI/Librerias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021cdb5b-b18f-4e76-9364-d86200628322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+---------+--------------------+\n|Crypto ID|       Current Date|Price USD|      usd_market_cap|\n+---------+-------------------+---------+--------------------+\n| ethereum|2023-11-02 04:39:32|  1855.61|2.232391994633327...|\n|   ripple|2023-11-02 04:39:32| 0.609428|3.269064577399247...|\n| litecoin|2023-11-02 04:39:32|    70.28|  5.18617978795294E9|\n|  cardano|2023-11-02 04:39:32| 0.308644|1.080036653545249...|\n+---------+-------------------+---------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Se crea una sesión Spark\n",
    "spark = SparkSession.builder.appName(\"CryptoData\").getOrCreate()\n",
    "\n",
    "# URL de la API de CoinGecko para obtener el precio de Bitcoin en dólares estadounidenses.\n",
    "url = \"https://api.coingecko.com/api/v3/simple/price\"\n",
    "\n",
    "# Lista de IDs de las criptomonedas que deseas consultar (puedes agregar más o cambiar las que necesites).\n",
    "crypto_ids = [\n",
    "    #\"bitcoin\",\n",
    "    \"ethereum\",\n",
    "    \"ripple\",\n",
    "    \"litecoin\",\n",
    "    \"cardano\",\n",
    "    # Se pueden agregar más monedas aca\n",
    "]\n",
    "# Formatea los IDs en una cadena separada por comas.\n",
    "ids_str = \",\".join(crypto_ids)\n",
    "params = {\n",
    "    \"ids\": ids_str,\n",
    "    \"vs_currencies\": \"usd\",\n",
    "    \"include_market_cap\": \"true\",\n",
    "    \"include_24hr_vol\": \"true\",\n",
    "    \"include_24hr_change\": \"true\"\n",
    "}\n",
    "\n",
    "# Realizar la solicitud GET a la API\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Comprobar si la solicitud se realizó con éxito\n",
    "if response.status_code == 200:\n",
    "    # La solicitud fue exitosa, analiza los datos JSON\n",
    "    data = response.json()\n",
    "    crypto_data = []\n",
    "    current_date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for crypto_id in crypto_ids:\n",
    "        price_usd = data[crypto_id]['usd']\n",
    "        last_price = data[crypto_id]['usd_market_cap']\n",
    "        crypto_data.append({'Crypto ID': crypto_id, 'Price USD': price_usd, 'usd_market_cap': last_price, 'Current Date':current_date})\n",
    "\n",
    "    # Crea un DataFrame a partir de los datos\n",
    "    df = spark.createDataFrame(crypto_data)\n",
    "\n",
    "    # Mostrar el DataFrame\n",
    "    df.show()\n",
    "else:\n",
    "    # La solicitud no fue exitosa. Puedes manejar el error según tus necesidades.\n",
    "    print(f\"Error en la solicitud. Código de estado: {response.status_code}\")\n",
    "\n",
    "# Detener la sesión Spark cuando hayas terminado\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98f28f6-a483-4dca-a4ee-2e2fd1d9e4c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Persistensia de Datos en DataLakeGen2**\n",
    "\n",
    "Parte 2: Integración de Servicios en Azure \n",
    "\n",
    "Escribe los datos procesados en la Parte 1.1 en un almacenamiento de datos persistente en \n",
    "Azure (por ejemplo, Azure Data Lake Storage, Azure Blob Storage, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1597debf-b3d1-4b62-9e78-9e19b216d763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuracion de la conexión \n",
    "configs = {\"fs.azure.account.auth.type\":\"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": \"<CLIENT_ID>\",\n",
    "           \"fs.azure.account.oauth2.client.secret\": \"<SECRET>\",\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<ENDPOINT>/oauth2/token\"}\n",
    "dbutils.fs.mount(\n",
    "    source = \"abfss://<CONTAINER>@<STORAGE_ACCOUNT>.dfs.core.windows.net/\",\n",
    "    mount_point = \"/mnt/<STORAGE_ACCOUNT>/<CONTAINER>\",\n",
    "    extra_configs = configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0dbd9cb-6253-4def-acd6-ff1fdfd9c7ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos procesados almacenados en Azure Data Lake Storage\n"
     ]
    }
   ],
   "source": [
    "# Se calcula la fecha y hora actual\n",
    "date_file = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Ruta destino mas fecha y hora actual\n",
    "ruta_target = \"/mnt/<STORAGE_ACCOUNT>/<CONTAINER>\" + date_file\n",
    "\n",
    "# Escribe el DataFrame en el almacenamiento de datos en Azure\n",
    "df.write.csv(ruta_target, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"Datos procesados almacenados en Azure Data Lake Storage\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL_sigloXXI",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
